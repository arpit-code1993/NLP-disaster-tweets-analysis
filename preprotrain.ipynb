{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AbstractLazySequence',\n",
       " 'AffixTagger',\n",
       " 'AlignedSent',\n",
       " 'Alignment',\n",
       " 'AnnotationTask',\n",
       " 'ApplicationExpression',\n",
       " 'Assignment',\n",
       " 'BigramAssocMeasures',\n",
       " 'BigramCollocationFinder',\n",
       " 'BigramTagger',\n",
       " 'BinaryMaxentFeatureEncoding',\n",
       " 'BlanklineTokenizer',\n",
       " 'BllipParser',\n",
       " 'BottomUpChartParser',\n",
       " 'BottomUpLeftCornerChartParser',\n",
       " 'BottomUpProbabilisticChartParser',\n",
       " 'Boxer',\n",
       " 'BrillTagger',\n",
       " 'BrillTaggerTrainer',\n",
       " 'CFG',\n",
       " 'CRFTagger',\n",
       " 'CfgReadingCommand',\n",
       " 'ChartParser',\n",
       " 'ChunkParserI',\n",
       " 'ChunkScore',\n",
       " 'Cistem',\n",
       " 'ClassifierBasedPOSTagger',\n",
       " 'ClassifierBasedTagger',\n",
       " 'ClassifierI',\n",
       " 'ConcordanceIndex',\n",
       " 'ConditionalExponentialClassifier',\n",
       " 'ConditionalFreqDist',\n",
       " 'ConditionalProbDist',\n",
       " 'ConditionalProbDistI',\n",
       " 'ConfusionMatrix',\n",
       " 'ContextIndex',\n",
       " 'ContextTagger',\n",
       " 'ContingencyMeasures',\n",
       " 'CoreNLPDependencyParser',\n",
       " 'CoreNLPParser',\n",
       " 'Counter',\n",
       " 'CrossValidationProbDist',\n",
       " 'DRS',\n",
       " 'DecisionTreeClassifier',\n",
       " 'DefaultTagger',\n",
       " 'DependencyEvaluator',\n",
       " 'DependencyGrammar',\n",
       " 'DependencyGraph',\n",
       " 'DependencyProduction',\n",
       " 'DictionaryConditionalProbDist',\n",
       " 'DictionaryProbDist',\n",
       " 'DiscourseTester',\n",
       " 'DrtExpression',\n",
       " 'DrtGlueReadingCommand',\n",
       " 'ELEProbDist',\n",
       " 'EarleyChartParser',\n",
       " 'Expression',\n",
       " 'FStructure',\n",
       " 'FeatDict',\n",
       " 'FeatList',\n",
       " 'FeatStruct',\n",
       " 'FeatStructReader',\n",
       " 'Feature',\n",
       " 'FeatureBottomUpChartParser',\n",
       " 'FeatureBottomUpLeftCornerChartParser',\n",
       " 'FeatureChartParser',\n",
       " 'FeatureEarleyChartParser',\n",
       " 'FeatureIncrementalBottomUpChartParser',\n",
       " 'FeatureIncrementalBottomUpLeftCornerChartParser',\n",
       " 'FeatureIncrementalChartParser',\n",
       " 'FeatureIncrementalTopDownChartParser',\n",
       " 'FeatureTopDownChartParser',\n",
       " 'FreqDist',\n",
       " 'HTTPPasswordMgrWithDefaultRealm',\n",
       " 'HeldoutProbDist',\n",
       " 'HiddenMarkovModelTagger',\n",
       " 'HiddenMarkovModelTrainer',\n",
       " 'HunposTagger',\n",
       " 'IBMModel',\n",
       " 'IBMModel1',\n",
       " 'IBMModel2',\n",
       " 'IBMModel3',\n",
       " 'IBMModel4',\n",
       " 'IBMModel5',\n",
       " 'ISRIStemmer',\n",
       " 'ImmutableMultiParentedTree',\n",
       " 'ImmutableParentedTree',\n",
       " 'ImmutableProbabilisticMixIn',\n",
       " 'ImmutableProbabilisticTree',\n",
       " 'ImmutableTree',\n",
       " 'IncrementalBottomUpChartParser',\n",
       " 'IncrementalBottomUpLeftCornerChartParser',\n",
       " 'IncrementalChartParser',\n",
       " 'IncrementalLeftCornerChartParser',\n",
       " 'IncrementalTopDownChartParser',\n",
       " 'Index',\n",
       " 'InsideChartParser',\n",
       " 'JSONTaggedDecoder',\n",
       " 'JSONTaggedEncoder',\n",
       " 'KneserNeyProbDist',\n",
       " 'LancasterStemmer',\n",
       " 'LaplaceProbDist',\n",
       " 'LazyConcatenation',\n",
       " 'LazyEnumerate',\n",
       " 'LazyIteratorList',\n",
       " 'LazyMap',\n",
       " 'LazySubsequence',\n",
       " 'LazyZip',\n",
       " 'LeftCornerChartParser',\n",
       " 'LidstoneProbDist',\n",
       " 'LineTokenizer',\n",
       " 'LogicalExpressionException',\n",
       " 'LongestChartParser',\n",
       " 'MLEProbDist',\n",
       " 'MWETokenizer',\n",
       " 'Mace',\n",
       " 'MaceCommand',\n",
       " 'MaltParser',\n",
       " 'MaxentClassifier',\n",
       " 'Model',\n",
       " 'MultiClassifierI',\n",
       " 'MultiParentedTree',\n",
       " 'MutableProbDist',\n",
       " 'NaiveBayesClassifier',\n",
       " 'NaiveBayesDependencyScorer',\n",
       " 'NgramAssocMeasures',\n",
       " 'NgramTagger',\n",
       " 'NonprojectiveDependencyParser',\n",
       " 'Nonterminal',\n",
       " 'OrderedDict',\n",
       " 'PCFG',\n",
       " 'Paice',\n",
       " 'ParallelProverBuilder',\n",
       " 'ParallelProverBuilderCommand',\n",
       " 'ParentedTree',\n",
       " 'ParserI',\n",
       " 'PerceptronTagger',\n",
       " 'PhraseTable',\n",
       " 'PorterStemmer',\n",
       " 'PositiveNaiveBayesClassifier',\n",
       " 'ProbDistI',\n",
       " 'ProbabilisticDependencyGrammar',\n",
       " 'ProbabilisticMixIn',\n",
       " 'ProbabilisticNonprojectiveParser',\n",
       " 'ProbabilisticProduction',\n",
       " 'ProbabilisticProjectiveDependencyParser',\n",
       " 'ProbabilisticTree',\n",
       " 'Production',\n",
       " 'ProjectiveDependencyParser',\n",
       " 'Prover9',\n",
       " 'Prover9Command',\n",
       " 'ProxyBasicAuthHandler',\n",
       " 'ProxyDigestAuthHandler',\n",
       " 'ProxyHandler',\n",
       " 'PunktSentenceTokenizer',\n",
       " 'QuadgramAssocMeasures',\n",
       " 'QuadgramCollocationFinder',\n",
       " 'RSLPStemmer',\n",
       " 'RTEFeatureExtractor',\n",
       " 'RUS_PICKLE',\n",
       " 'RandomChartParser',\n",
       " 'RangeFeature',\n",
       " 'ReadingCommand',\n",
       " 'RecursiveDescentParser',\n",
       " 'RegexpChunkParser',\n",
       " 'RegexpParser',\n",
       " 'RegexpStemmer',\n",
       " 'RegexpTagger',\n",
       " 'RegexpTokenizer',\n",
       " 'ReppTokenizer',\n",
       " 'ResolutionProver',\n",
       " 'ResolutionProverCommand',\n",
       " 'SExprTokenizer',\n",
       " 'SLASH',\n",
       " 'Senna',\n",
       " 'SennaChunkTagger',\n",
       " 'SennaNERTagger',\n",
       " 'SennaTagger',\n",
       " 'SequentialBackoffTagger',\n",
       " 'ShiftReduceParser',\n",
       " 'SimpleGoodTuringProbDist',\n",
       " 'SklearnClassifier',\n",
       " 'SlashFeature',\n",
       " 'SnowballStemmer',\n",
       " 'SpaceTokenizer',\n",
       " 'StackDecoder',\n",
       " 'StanfordNERTagger',\n",
       " 'StanfordPOSTagger',\n",
       " 'StanfordSegmenter',\n",
       " 'StanfordTagger',\n",
       " 'StemmerI',\n",
       " 'SteppingChartParser',\n",
       " 'SteppingRecursiveDescentParser',\n",
       " 'SteppingShiftReduceParser',\n",
       " 'SyllableTokenizer',\n",
       " 'TYPE',\n",
       " 'TabTokenizer',\n",
       " 'TableauProver',\n",
       " 'TableauProverCommand',\n",
       " 'TaggerI',\n",
       " 'TestGrammar',\n",
       " 'Text',\n",
       " 'TextCat',\n",
       " 'TextCollection',\n",
       " 'TextTilingTokenizer',\n",
       " 'TnT',\n",
       " 'TokenSearcher',\n",
       " 'ToktokTokenizer',\n",
       " 'TopDownChartParser',\n",
       " 'TransitionParser',\n",
       " 'Tree',\n",
       " 'TreebankWordTokenizer',\n",
       " 'Trie',\n",
       " 'TrigramAssocMeasures',\n",
       " 'TrigramCollocationFinder',\n",
       " 'TrigramTagger',\n",
       " 'TweetTokenizer',\n",
       " 'TypedMaxentFeatureEncoding',\n",
       " 'Undefined',\n",
       " 'UniformProbDist',\n",
       " 'UnigramTagger',\n",
       " 'UnsortedChartParser',\n",
       " 'Valuation',\n",
       " 'Variable',\n",
       " 'ViterbiParser',\n",
       " 'WekaClassifier',\n",
       " 'WhitespaceTokenizer',\n",
       " 'WittenBellProbDist',\n",
       " 'WordNetLemmatizer',\n",
       " 'WordPunctTokenizer',\n",
       " '__author__',\n",
       " '__author_email__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__classifiers__',\n",
       " '__copyright__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__keywords__',\n",
       " '__license__',\n",
       " '__loader__',\n",
       " '__longdescr__',\n",
       " '__maintainer__',\n",
       " '__maintainer_email__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__url__',\n",
       " '__version__',\n",
       " 'absolute_import',\n",
       " 'accuracy',\n",
       " 'add_logs',\n",
       " 'agreement',\n",
       " 'align',\n",
       " 'alignment_error_rate',\n",
       " 'aline',\n",
       " 'api',\n",
       " 'app',\n",
       " 'apply_features',\n",
       " 'approxrand',\n",
       " 'arity',\n",
       " 'association',\n",
       " 'bigrams',\n",
       " 'binary_distance',\n",
       " 'binary_search_file',\n",
       " 'binding_ops',\n",
       " 'bisect',\n",
       " 'blankline_tokenize',\n",
       " 'bleu',\n",
       " 'bleu_score',\n",
       " 'bllip',\n",
       " 'boolean_ops',\n",
       " 'boxer',\n",
       " 'bracket_parse',\n",
       " 'breadth_first',\n",
       " 'brill',\n",
       " 'brill_trainer',\n",
       " 'build_opener',\n",
       " 'call_megam',\n",
       " 'casual',\n",
       " 'casual_tokenize',\n",
       " 'ccg',\n",
       " 'chain',\n",
       " 'chart',\n",
       " 'chat',\n",
       " 'choose',\n",
       " 'chunk',\n",
       " 'cistem',\n",
       " 'class_types',\n",
       " 'classify',\n",
       " 'clause',\n",
       " 'clean_html',\n",
       " 'clean_url',\n",
       " 'cluster',\n",
       " 'collections',\n",
       " 'collocations',\n",
       " 'combinations',\n",
       " 'compat',\n",
       " 'config_java',\n",
       " 'config_megam',\n",
       " 'config_weka',\n",
       " 'conflicts',\n",
       " 'confusionmatrix',\n",
       " 'conllstr2tree',\n",
       " 'conlltags2tree',\n",
       " 'corenlp',\n",
       " 'corpus',\n",
       " 'crf',\n",
       " 'custom_distance',\n",
       " 'data',\n",
       " 'decisiontree',\n",
       " 'decorator',\n",
       " 'decorators',\n",
       " 'defaultdict',\n",
       " 'demo',\n",
       " 'dependencygraph',\n",
       " 'deque',\n",
       " 'discourse',\n",
       " 'distance',\n",
       " 'download',\n",
       " 'download_gui',\n",
       " 'download_shell',\n",
       " 'downloader',\n",
       " 'draw',\n",
       " 'drt',\n",
       " 'earleychart',\n",
       " 'edit_distance',\n",
       " 'edit_distance_align',\n",
       " 'elementtree_indent',\n",
       " 'entropy',\n",
       " 'equality_preds',\n",
       " 'evaluate',\n",
       " 'evaluate_sents',\n",
       " 'everygrams',\n",
       " 'extract_rels',\n",
       " 'extract_test_sentences',\n",
       " 'f_measure',\n",
       " 'featstruct',\n",
       " 'featurechart',\n",
       " 'filestring',\n",
       " 'find',\n",
       " 'flatten',\n",
       " 'fractional_presence',\n",
       " 'getproxies',\n",
       " 'ghd',\n",
       " 'glue',\n",
       " 'grammar',\n",
       " 'guess_encoding',\n",
       " 'help',\n",
       " 'hmm',\n",
       " 'hunpos',\n",
       " 'ibm1',\n",
       " 'ibm2',\n",
       " 'ibm3',\n",
       " 'ibm4',\n",
       " 'ibm5',\n",
       " 'ibm_model',\n",
       " 'ieerstr2tree',\n",
       " 'improved_close_quote_regex',\n",
       " 'improved_open_quote_regex',\n",
       " 'improved_open_single_quote_regex',\n",
       " 'improved_punct_regex',\n",
       " 'in_idle',\n",
       " 'induce_pcfg',\n",
       " 'inference',\n",
       " 'infile',\n",
       " 'inspect',\n",
       " 'install_opener',\n",
       " 'internals',\n",
       " 'interpret_sents',\n",
       " 'interval_distance',\n",
       " 'invert_dict',\n",
       " 'invert_graph',\n",
       " 'is_rel',\n",
       " 'islice',\n",
       " 'isri',\n",
       " 'jaccard_distance',\n",
       " 'json_tags',\n",
       " 'jsontags',\n",
       " 'lancaster',\n",
       " 'lazyimport',\n",
       " 'lfg',\n",
       " 'line_tokenize',\n",
       " 'linearlogic',\n",
       " 'lm',\n",
       " 'load',\n",
       " 'load_parser',\n",
       " 'locale',\n",
       " 'log_likelihood',\n",
       " 'logic',\n",
       " 'mace',\n",
       " 'malt',\n",
       " 'map_tag',\n",
       " 'mapping',\n",
       " 'masi_distance',\n",
       " 'maxent',\n",
       " 'megam',\n",
       " 'memoize',\n",
       " 'meteor',\n",
       " 'meteor_score',\n",
       " 'metrics',\n",
       " 'misc',\n",
       " 'mwe',\n",
       " 'naivebayes',\n",
       " 'ne_chunk',\n",
       " 'ne_chunk_sents',\n",
       " 'ngrams',\n",
       " 'nonprojectivedependencyparser',\n",
       " 'nonterminals',\n",
       " 'numpy',\n",
       " 'os',\n",
       " 'pad_sequence',\n",
       " 'paice',\n",
       " 'parse',\n",
       " 'parse_sents',\n",
       " 'pchart',\n",
       " 'perceptron',\n",
       " 'pk',\n",
       " 'porter',\n",
       " 'pos_tag',\n",
       " 'pos_tag_sents',\n",
       " 'positivenaivebayes',\n",
       " 'pprint',\n",
       " 'pr',\n",
       " 'precision',\n",
       " 'presence',\n",
       " 'print_function',\n",
       " 'print_string',\n",
       " 'probability',\n",
       " 'projectivedependencyparser',\n",
       " 'prover9',\n",
       " 'punkt',\n",
       " 'py25',\n",
       " 'py26',\n",
       " 'py27',\n",
       " 'pydoc',\n",
       " 'python_2_unicode_compatible',\n",
       " 'raise_unorderable_types',\n",
       " 'ranks_from_scores',\n",
       " 'ranks_from_sequence',\n",
       " 're',\n",
       " 're_show',\n",
       " 'read_grammar',\n",
       " 'read_logic',\n",
       " 'read_valuation',\n",
       " 'recall',\n",
       " 'recursivedescent',\n",
       " 'regexp',\n",
       " 'regexp_span_tokenize',\n",
       " 'regexp_tokenize',\n",
       " 'register_tag',\n",
       " 'relextract',\n",
       " 'repp',\n",
       " 'resolution',\n",
       " 'ribes',\n",
       " 'ribes_score',\n",
       " 'root_semrep',\n",
       " 'rslp',\n",
       " 'rte_classifier',\n",
       " 'rte_classify',\n",
       " 'rte_features',\n",
       " 'rtuple',\n",
       " 'scikitlearn',\n",
       " 'scores',\n",
       " 'segmentation',\n",
       " 'sem',\n",
       " 'senna',\n",
       " 'sent_tokenize',\n",
       " 'sequential',\n",
       " 'set2rel',\n",
       " 'set_proxy',\n",
       " 'sexpr',\n",
       " 'sexpr_tokenize',\n",
       " 'shiftreduce',\n",
       " 'simple',\n",
       " 'sinica_parse',\n",
       " 'skipgrams',\n",
       " 'skolemize',\n",
       " 'slice_bounds',\n",
       " 'snowball',\n",
       " 'sonority_sequencing',\n",
       " 'spearman',\n",
       " 'spearman_correlation',\n",
       " 'stack_decoder',\n",
       " 'stanford',\n",
       " 'stanford_segmenter',\n",
       " 'stem',\n",
       " 'str2tuple',\n",
       " 'string_span_tokenize',\n",
       " 'string_types',\n",
       " 'subprocess',\n",
       " 'subsumes',\n",
       " 'sum_logs',\n",
       " 'sys',\n",
       " 'tableau',\n",
       " 'tadm',\n",
       " 'tag',\n",
       " 'tagset_mapping',\n",
       " 'tagstr2tree',\n",
       " 'tbl',\n",
       " 'text',\n",
       " 'text_type',\n",
       " 'textcat',\n",
       " 'texttiling',\n",
       " 'textwrap',\n",
       " 'tkinter',\n",
       " 'tnt',\n",
       " 'tokenize',\n",
       " 'tokenwrap',\n",
       " 'toktok',\n",
       " 'toolbox',\n",
       " 'total_ordering',\n",
       " 'transitionparser',\n",
       " 'transitive_closure',\n",
       " 'translate',\n",
       " 'tree',\n",
       " 'tree2conllstr',\n",
       " 'tree2conlltags',\n",
       " 'treebank',\n",
       " 'treetransforms',\n",
       " 'trigrams',\n",
       " 'tuple2str',\n",
       " 'types',\n",
       " 'unify',\n",
       " 'unique_list',\n",
       " 'untag',\n",
       " 'usage',\n",
       " 'util',\n",
       " 'version_file',\n",
       " 'version_info',\n",
       " 'viterbi',\n",
       " 'weka',\n",
       " 'windowdiff',\n",
       " 'word_tokenize',\n",
       " 'wordnet',\n",
       " 'wordpunct_tokenize',\n",
       " 'wsd']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows and columns in the dataset is (7613, 5)\n"
     ]
    }
   ],
   "source": [
    "#shape of the data\n",
    "\n",
    "print('number of rows and columns in the dataset is {}'.format(df_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 7613 rows \n",
      " 3271 are real disasters \n",
      " 4342 are not real disasters\n",
      " in train dataset\n"
     ]
    }
   ],
   "source": [
    "#How many of them are real and non real disasters\n",
    "print(\"Out of {} rows \\n {} are real disasters \\n {} are not real disasters\\n in train dataset\".format(len(df_train),\n",
    "                                                       len(df_train[df_train['target']==1]),\n",
    "                                                       len(df_train[df_train['target']==0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in column-Keyword is 61\n",
      "Number of missing values in column-location is 2533\n",
      "Number of missing values in column-text is 0\n",
      "Number of missing values in column-target is 0\n"
     ]
    }
   ],
   "source": [
    "#How many missing data per each column\n",
    "\n",
    "print('Number of missing values in column-Keyword is {}'.format(df_train['keyword'].isnull().sum()))\n",
    "print('Number of missing values in column-location is {}'.format(df_train['location'].isnull().sum()))\n",
    "print('Number of missing values in column-text is {}'.format(df_train['text'].isnull().sum()))\n",
    "print('Number of missing values in column-target is {}'.format(df_train['target'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "real_fake = [len(df_train[df_train['target']==1]),len(df_train[df_train['target']==0])]\n",
    "activities = ['Real', 'Not Real']\n",
    "colors = ['r', 'g']\n",
    "plt.pie(real_fake, labels=activities, colors=colors, startangle=90, autopct='%.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "custom_stop_list = []\n",
    "stopword_set = nltk.corpus.stopwords.words('english')+custom_stop_list+['url']\n",
    "wn = nltk.WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(iter):\n",
    "        # remove extra space\n",
    "        regex_ws=re.compile(\"\\s+\")\n",
    "        ret=regex_ws.sub(\" \",iter)\n",
    "        #ret=ret.replace(\"&amp;\",\"&\").replace(\"&lt;\",\"<\").replace(\"&gt;\",\">\")\n",
    "        #text=\"\".join([word for word in text if word not in string.punctuation])\n",
    "        \n",
    "        #Replace URL\n",
    "        regexp=\"(https?:\\/\\/(?:www\\.|(?!www)|(?:xmlns\\.))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\"\n",
    "        ret=re.sub(regexp,\"url\",ret)\n",
    "       \n",
    "        #replace @addresses\n",
    "        regexp='@[A-z0-9_]+'\n",
    "        ret=re.sub(regexp,\"@twitterhandle\",ret)\n",
    "        \n",
    "        #Split on punctuations\n",
    "        ret1=re.split(\"[,_, \\<>!\\?\\.:\\n\\\"=*/]+\",ret)\n",
    "        \n",
    "        #Remove Stopwords\n",
    "        ret2=[word for word in ret1 if word not in stopword_set]\n",
    "        ret2=\" \".join(ret2)\n",
    "        \n",
    "        #Remove  numbers\n",
    "        ret2=re.sub(r\"(\\s\\d+)\",\" \",ret2)\n",
    "        \n",
    "        return ret2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['body_text_nostop']=df_train['text'].apply(lambda x: preProcess(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text_nopunct=\"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['body_text_nopunct']=df_train['body_text_nostop'].apply(lambda x:remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>body_text_nostop</th>\n",
       "      <th>body_text_nopunct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deeds reason #earthquake may allah forgive us</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>residents asked 'shelter place' notified offic...</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13  people receive #wildfires evacuation order...</td>\n",
       "      <td>13  people receive wildfires evacuation orders...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby #alaska smoke #wildfires p...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                   body_text_nostop  \\\n",
       "0       1      deeds reason #earthquake may allah forgive us   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  residents asked 'shelter place' notified offic...   \n",
       "3       1  13  people receive #wildfires evacuation order...   \n",
       "4       1  got sent photo ruby #alaska smoke #wildfires p...   \n",
       "\n",
       "                                   body_text_nopunct  \n",
       "0       deeds reason earthquake may allah forgive us  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  residents asked shelter place notified officer...  \n",
       "3  13  people receive wildfires evacuation orders...  \n",
       "4  got sent photo ruby alaska smoke wildfires pou...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword: 0.008% missing values\n",
      "location: 0.3327% missing values\n"
     ]
    }
   ],
   "source": [
    "features_nan=[feature for feature in df_train.columns if df_train[feature].isnull().sum()>1 and df_train[feature].dtypes=='O']\n",
    "\n",
    "for feature in features_nan:\n",
    "    print(\"{}: {}% missing values\".format(feature,np.round(df_train[feature].isnull().mean(),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                   0\n",
       "keyword              0\n",
       "location             0\n",
       "text                 0\n",
       "target               0\n",
       "body_text_nostop     0\n",
       "body_text_nopunct    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_miss_value(df_train,features_nan):\n",
    "    data=df_train.copy()\n",
    "    data[features_nan]=data[features_nan].fillna('missing')\n",
    "    return data\n",
    "\n",
    "\n",
    "df_train=replace_miss_value(df_train,features_nan)\n",
    "\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>body_text_nostop</th>\n",
       "      <th>body_text_nopunct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deeds reason #earthquake may allah forgive us</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>residents asked 'shelter place' notified offic...</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13  people receive #wildfires evacuation order...</td>\n",
       "      <td>13  people receive wildfires evacuation orders...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>missing</td>\n",
       "      <td>missing</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby #alaska smoke #wildfires p...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  keyword location                                               text  \\\n",
       "0   1  missing  missing  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4  missing  missing             Forest fire near La Ronge Sask. Canada   \n",
       "2   5  missing  missing  All residents asked to 'shelter in place' are ...   \n",
       "3   6  missing  missing  13,000 people receive #wildfires evacuation or...   \n",
       "4   7  missing  missing  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                   body_text_nostop  \\\n",
       "0       1      deeds reason #earthquake may allah forgive us   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  residents asked 'shelter place' notified offic...   \n",
       "3       1  13  people receive #wildfires evacuation order...   \n",
       "4       1  got sent photo ruby #alaska smoke #wildfires p...   \n",
       "\n",
       "                                   body_text_nopunct  \n",
       "0       deeds reason earthquake may allah forgive us  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  residents asked shelter place notified officer...  \n",
       "3  13  people receive wildfires evacuation orders...  \n",
       "4  got sent photo ruby alaska smoke wildfires pou...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(2, 6),\n",
    "    norm='l2',\n",
    "    min_df=0,\n",
    "    smooth_idf=False,preprocessor=preProcess,\n",
    "    max_features=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_text = df_train['body_text_nopunct']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=30000,\n",
       "                min_df=0, ngram_range=(2, 6), norm='l2',\n",
       "                preprocessor=<function preProcess at 0x0000027E1DF7F730>,\n",
       "                smooth_idf=False, stop_words='english', strip_accents='unicode',\n",
       "                sublinear_tf=True, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "char_vectorizer.fit(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char_features = char_vectorizer.transform(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 30000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_char_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse matrix\n",
    "X_Tfidf_df=pd.DataFrame(train_char_features.toarray())\n",
    "X_Tfidf_df.columns = char_vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 30000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Tfidf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X_Tfidf_df\n",
    "y=df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 30000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.DataFrame(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcess the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('test.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['body_text_nostop']=df_test['text'].apply(lambda x: preProcess(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['body_text_nopunct']=df_test['body_text_nostop'].apply(lambda x:remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = df_test['body_text_nopunct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_char_features = char_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse matrix\n",
    "X_Tfidf_dft=pd.DataFrame(test_char_features.toarray())\n",
    "X_Tfidf_dft.columns = char_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=X_Tfidf_dft\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\K T KUMAR\\Anaconda33\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\K T KUMAR\\Anaconda33\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\K T KUMAR\\Anaconda33\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 61.70%\n"
     ]
    }
   ],
   "source": [
    "skfold = StratifiedKFold(n_splits=3, random_state=100)\n",
    "model_skfold = LogisticRegression()\n",
    "results_skfold = model_selection.cross_val_score(model_skfold, X, y, cv=skfold,scoring='f1')\n",
    "print(\"F1 score: %.2f%%\" % (results_skfold.mean()*100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\K T KUMAR\\Anaconda33\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7409879839786382"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,random_state =5)\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "metrics.f1_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pd.DataFrame(Y_pred)\n",
    "datasets=pd.concat([sub_df['id'],pred],axis=1)\n",
    "datasets.columns=['id','target']\n",
    "datasets.to_csv('sample_submission2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7578781512605042"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,random_state =5)\n",
    "knnclassifier = KNeighborsClassifier(n_neighbors=5)\n",
    "knnclassifier.fit(x_train,y_train)\n",
    "y_pred = knnclassifier.predict(x_test)\n",
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.692871419053964"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = knnclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df=pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pd.DataFrame(Y_pred)\n",
    "datasets=pd.concat([sub_df['id'],pred],axis=1)\n",
    "datasets.columns=['id','target']\n",
    "datasets.to_csv('sample_submission1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_curve\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split,cross_val_score\n",
    " \n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6875871687587168"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y,random_state =5)\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(x_train,y_train)\n",
    "y_pred = xgb_model.predict(x_test)\n",
    "metrics.f1_score(y_test,y_pred)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pd.DataFrame(Y_pred)\n",
    "datasets=pd.concat([sub_df['id'],pred],axis=1)\n",
    "datasets.columns=['id','target']\n",
    "datasets.to_csv('sample_submission3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation scores\n",
    "f1_scores = model_selection.cross_val_score(xgb_model, X, y, cv=skfold,scoring='f1')\n",
    "print(\"F1-score = \",f1_scores,\" Mean F1 score = \",np.mean(f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
